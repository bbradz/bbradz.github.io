<!DOCTYPE html>
<html lang="en" data-theme="dark">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="stylesheet" href="../styles.css" />
    <title>Transformer FLOPs</title>
  </head>
  <body>
    <!-- Header -->
    <header class="header">
      <div class="logo-section">
        <a href="/about" class="logo">BBradz</a>
        <button
          class="theme-toggle"
          onclick="toggleTheme()"
          aria-label="Toggle theme"
        >

          <svg
            class="sun-icon"
            viewBox="0 0 24 24"
            fill="none"
            xmlns="http://www.w3.org/2000/svg"
          >
            <circle cx="12" cy="12" r="4"></circle>
            <path d="M12 2v2"></path>
            <path d="M12 20v2"></path>
            <path d="M4.93 4.93l1.41 1.41"></path>
            <path d="M17.66 17.66l1.41 1.41"></path>
            <path d="M2 12h2"></path>
            <path d="M20 12h2"></path>
            <path d="M6.34 17.66l-1.41 1.41"></path>
            <path d="M19.07 4.93l-1.41 1.41"></path>
          </svg>

          <svg
            class="moon-icon"
            viewBox="0 0 24 24"
            fill="none"
            xmlns="http://www.w3.org/2000/svg"
          >
            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
          </svg>
        </button>
      </div>
      <nav class="nav-links">
        <a href="/posts" class="nav-link">Posts</a>
        <a href="../reading.html" class="nav-link">Library</a>
        <a href="/profile" class="nav-link">Psychological Profile</a>
        <a href="/about" class="nav-link">About</a>
      </nav>
    </header>

    <!-- Article -->
    <div class="container">
      <h1 id="A">Transformer FLOPs</h1>

      <div class="header-content">
        <div class="header-left">
          <div class="metadata">Benjamin Bradley, Tue Dec 17 2024 • 69 min read (10K words)</div>

          <div class="tags">
            <span class="tag">optimizers</span>
            <span class="tag">distributed systems</span>
          </div>
        </div>
        <a href="#" class="back-link" onclick="history.back(); return false;">Back</a>

      </div>

      <div class="discussion-links">
        <div>• <a href="#">Source code for this article on GitHub</a></div>
        <div>• <a href="#">Discussion on Hacker News</a></div>
        <div>• <a href="#">Discussion on Reddit</a></div>
        <div>• <a href="#">Discussion on Twitter</a></div>
      </div>

      <div class="toc-container" onclick="toggleTOC()">
        <div class="toc-header">
          <span>▶</span>
          <span><b>Table of Contents</b></span>
        </div>
        <div class="toc-content" id="toc">
          <!-- Add your actual table of contents here -->
          <a href="#introduction">Introduction</a>
          <a href="#components">&nbsp;&nbsp;Key Components</a>
          <a href="#architecture">&nbsp;&nbsp;Architecture</a>
          <a href="#examples">Examples</a>
          <a href="#challenges">Challenges</a>
          <a href="#conclusion">Conclusion</a>
        </div>
      </div>

      <p>
        Counting the number of floating-point operations (FLOPs) in Transformers
        is a useful way to estimate compute requirements and measure efficiency.
        As training runs get larger and larger (thus more expensive) it becomes
        more important to understand how many FLOPs we need to do and how well
        we utilize our hardware.
      </p>

      <h2 id="B">Counting FLOPs in Transformers</h2>

      <p>
        One commonly used method for counting FLOPs is from the OpenAI scaling
        law paper<sup>1</sup>
        which uses:
      </p>

      <div class="formula-text">C<sub>forward+backward</sub> ≈ 6N</div>

      <p>
        for estimating the number of FLOPs per token during the training of a
        decoder-only Transformer where N is the number of non-embedding
        parameters in the model. To derive this we can look at the table they
        provide for FLOP counts of various components of the model for the
        forward pass.
      </p>

      <table>
        <thead>
          <tr>
            <th>Operation</th>
            <th>Parameters</th>
            <th>FLOPs per Token</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Embed</td>
            <td>(n<sub>vocab</sub> + n<sub>ctx</sub>)d<sub>model</sub></td>
            <td>4d<sub>model</sub></td>
          </tr>
          <tr>
            <td>Attention: QKV</td>
            <td>n<sub>layer</sub>d<sub>model</sub>3d<sub>attn</sub></td>
            <td>2n<sub>layer</sub>d<sub>model</sub>3d<sub>attn</sub></td>
          </tr>
          <tr>
            <td>Attention: Mask</td>
            <td>−</td>
            <td>2n<sub>layer</sub>n<sub>ctx</sub>d<sub>attn</sub></td>
          </tr>
          <tr>
            <td>Attention: Project</td>
            <td>n<sub>layer</sub>d<sub>attn</sub>d<sub>model</sub></td>
            <td>2n<sub>layer</sub>d<sub>attn</sub>d<sub>model</sub></td>
          </tr>
          <tr>
            <td>Feedforward</td>
            <td>n<sub>layer</sub>2d<sub>model</sub>d<sub>ff</sub></td>
            <td>2n<sub>layer</sub>2d<sub>model</sub>d<sub>ff</sub></td>
          </tr>
          <tr>
            <td>De-embed</td>
            <td>−</td>
            <td>2d<sub>model</sub>n<sub>vocab</sub></td>
          </tr>
          <tr>
            <td>Total (Non-Embedding)</td>
            <td>
              N = 2d<sub>model</sub>n<sub>layer</sub>(2d<sub>attn</sub> +
              d<sub>ff</sub>)
            </td>
            <td>
              C<sub>forward</sub> = 2N + 2n<sub>layer</sub>n<sub>ctx</sub>d<sub
                >attn</sub
              >
            </td>
          </tr>
        </tbody>
      </table>

      <p>
        Nonlinearities, biases, normalizations, and residuals are not counted as
        they turn out to be negligible. Let's explain each operation and
        variable here:
      </p>

      <div class="indent-list">
        <div class="bullet">
          <span class="monospace">Embed</span>: learned token embeddings and
          learned positional embeddings.
          <div class="sub-bullet">
            d<sub>model</sub> is the dimensionality of the residual stream.
          </div>
        </div>

        <div class="bullet">
          <span class="monospace">Attention: QKV</span>: linear layer in
          multi-head self-attention to project input into queries, keys, and
          values.
          <div class="sub-bullet">
            n<sub>layer</sub> is the number of layers.
          </div>
          <div class="sub-bullet">
            d<sub>attn</sub> is the dimensionality of the output of multi-headed
            attention which is equal to d<sub>key</sub>n<sub>heads</sub>.
          </div>
          <div class="sub-bullet">
            d<sub>key</sub> is the dimension of the key, query, and value
            projections.
          </div>
          <div class="sub-bullet">
            n<sub>heads</sub> is the number of attention heads in a layer.
          </div>
          <div class="sub-bullet">
            In practice, Transformers are implemented such that d<sub>attn</sub>
            = d<sub>model</sub>.
          </div>
        </div>

        <div class="bullet">
          <span class="monospace">Attention: Mask</span>: dot-product between
          query and keys.
          <div class="sub-bullet">
            n<sub>ctx</sub> is the context/sequence length.
          </div>
        </div>

        <div class="bullet">
          <span class="monospace">Attention: Project</span>: linear layer to
          project concatenated attention heads output to d<sub>model</sub>.
        </div>

        <div class="bullet">
          <span class="monospace">Feedforward</span>: two linear layers in the
          MLP block.
        </div>
      </div>

      <p>
        For example, when using the fp16/bf16 formats an A100 has a theoretical
        peak of 312 teraFLOPS (TFLOPS)<sup>6</sup>. If we use the 6N estimate
        for forward+backward FLOPs and are training a 125M parameter model using
        an A100 and we have throughput of 200,000 tokens per second then our MFU
        is
      </p>

      <div class="math-block">
        MFU = 6ND/P<br /><br />
        = (6 · 125 × 10<sup>6</sup> · 200 × 10<sup>3</sup>)/(312 ×
        10<sup>12</sup>)<br /><br />
        = 0.48
      </div>

      <p>
        Which means we achieved 48% of the theoretical peak FLOPS. With this
        method if we are doing something like activation checkpointing this will
        hurt our MFU, but it wouldn't necessarily hurt our HFU. Conversely, if
        we could get away with not using activation checkpointing then our MFU
        would improve. This isn't to say HFU is worthless though. If we need to
        use activation checkpointing, HFU can help gauge the efficiency of our
        rematerialization implementation. In the long run though we should
        generally want to optimize for MFU and it also allows fairer comparison
        of efficiency across different training set ups.
      </p>

      <p>
        In practice, the range of MFU for language models can vary widely
        depending on model size, hardware<sup>7</sup>, and implementation
        details, but generally range between 10-65%<sup>8</sup> <sup>9</sup>
        <sup>10</sup>.
      </p>

      <h2 id="C">Scaling of FLOPs</h2>

      <p>
        As we scale the size of Transformers it can be useful to know how
        different components of the model contribute to the computational
        cost<sup>11</sup> <sup>12</sup> <sup>13</sup>. Let's look at how the
        FLOPs of these components (using the operations in the DeepMind table)
        contribute to the total compute as we scale the model.
      </p>

      <div class="graph-container">
        <!-- Replace src with your actual image path -->
        <img
          src="pics/epq-meme.jpg"
          alt="Graph showing scaling of FLOPs contribution with sequence length of 4096"
        />
      </div>

      <h2 id="D">Appendix B: FLOPs counting methods in code</h2>

      <p>OpenAI's counting method in code:</p>

      <div class="code-container" onmousemove="handleMouseMove(event)">
        <div class="code-content">
          <button class="copy-button" onclick="copyCode(event)">
            <svg
              id="copy-icon"
              xmlns="http://www.w3.org/2000/svg"
              width="20"
              height="20"
              viewBox="0 0 24 24"
              fill="none"
              stroke="currentColor"
              stroke-width="2"
              stroke-linecap="round"
              stroke-linejoin="round"
            >
              <rect x="9" y="9" width="13" height="13" rx="2" ry="2"></rect>
              <path
                d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"
              ></path>
            </svg>
            <svg
              id="check-icon"
              xmlns="http://www.w3.org/2000/svg"
              width="20"
              height="20"
              viewBox="0 0 24 24"
              fill="none"
              stroke="currentColor"
              stroke-width="2"
              stroke-linecap="round"
              stroke-linejoin="round"
              style="display: none"
            >
              <polyline points="20 6 9 17 4 12"></polyline>
            </svg>
          </button>
          <pre><code><span class="keyword">def</span> <span class="function">openai_flops_per_token</span>(<span class="parameter">n_layers</span>, <span class="parameter">n_heads</span>, <span class="parameter">d_model</span>, <span class="parameter">n_ctx</span>, <span class="parameter">n_vocab</span>, <span class="parameter">ff_ratio</span>):
        <span class="comment">"""Open AI method for forward pass FLOPs counting of decoder-only Transformer"""</span>
        
        d_attn = d_model // n_heads
        d_ff = d_model * ff_ratio
        
        embeddings = <span class="number">4</span> * d_model
        attn_qkv = <span class="number">2</span> * n_layers * d_model * <span class="number">3</span> * (d_attn * n_heads)
        attn_mask = <span class="number">2</span> * n_layers * n_ctx * (d_attn * n_heads)
        attn_project = <span class="number">2</span> * n_layers * (d_attn * n_heads) * d_model
        ff = <span class="number">2</span> * n_layers * <span class="number">2</span> * d_model * d_ff
        logits = <span class="number">2</span> * d_model * n_vocab
        
        <span class="keyword">return</span> embeddings + attn_qkv + attn_mask + attn_project + ff + logits</code></pre>
        </div>
      </div>

      <!-- Sources -->
      <h2 id="E">References</h2>
      
      <div class="bullet">
        <span>1.</span>
        Kaplan, J., McCandlish, S., Henighan, T., Brown, T.B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., Amodei, D. 
        <a href="#" style="color: inherit; text-decoration: underline;">Scaling Laws for Neural Language Models</a>. 
        arXiv, 2020.
      </div>

      <div class="bullet">
        <span>2.</span>
        Bahdanau, D., 
        <a href="#" style="color: inherit; text-decoration: underline;">The FLOPs Calculus of Language Model Training</a>. 
        See section <i>Derivation of Transformer FLOPs Equation</i> for a good explanation of each forward+backward FLOP for a single weight. 2022.
      </div>

      <p>To cite this blog post, please use:</p>

      <div
        class="code-container citation-code"
        onmousemove="handleMouseMove(event, 'citation-copy-button')"
      >
        <div class="code-content">
          <button
            id="citation-copy-button"
            class="copy-button"
            onclick="copyCitation(event)"
          >
            <svg
              id="citation-copy-icon"
              xmlns="http://www.w3.org/2000/svg"
              width="20"
              height="20"
              viewBox="0 0 24 24"
              fill="none"
              stroke="currentColor"
              stroke-width="2"
              stroke-linecap="round"
              stroke-linejoin="round"
            >
              <rect x="9" y="9" width="13" height="13" rx="2" ry="2"></rect>
              <path
                d="M5 15H4a2 2 0 0 1-2-2V4a2 2 0 0 1 2-2h9a2 2 0 0 1 2 2v1"
              ></path>
            </svg>
            <svg
              id="citation-check-icon"
              xmlns="http://www.w3.org/2000/svg"
              width="20"
              height="20"
              viewBox="0 0 24 24"
              fill="none"
              stroke="currentColor"
              stroke-width="2"
              stroke-linecap="round"
              stroke-linejoin="round"
              style="display: none"
            >
              <polyline points="20 6 9 17 4 12"></polyline>
            </svg>
          </button>
          <pre><code><span class="keyword">@misc</span><span class="operator">{</span>transformer-math-eleutherai,
            <span class="parameter">title</span> <span class="operator">=</span> <span class="string">{Transformer Math 101}</span>,
            <span class="parameter">author</span> <span class="operator">=</span> <span class="string">{Anthony, Quentin and Biderman, Stella and Schoelkopf, Hailey}</span>,
            <span class="parameter">howpublished</span> <span class="operator">=</span> <span class="string">\url{bbradz.github.io/}</span>,
            <span class="parameter">year</span> <span class="operator">=</span> <span class="string">{2024}</span>
            <span class="operator">}</span></code></pre>
        </div>
      </div>
    </div>
    </div>

    <!-- Scroll-to-top Button -->
    <div
      id="scrollToTop"
      class="fixed bottom-6 right-6 opacity-0 transition-opacity duration-200 cursor-pointer"
      onclick="scrollToTop()"
    >
      <div class="bg-zinc-800 hover:bg-zinc-700 rounded-full p-3 shadow-lg">
        <svg
          xmlns="http://www.w3.org/2000/svg"
          width="24"
          height="24"
          viewBox="0 0 24 24"
          fill="none"
          stroke="currentColor"
          stroke-width="2"
          stroke-linecap="round"
          stroke-linejoin="round"
          class="text-zinc-200"
        >
          <path d="M18 15l-6-6-6 6" />
        </svg>
      </div>
    </div>

    <!-- Sidebar TOC -->
    <div id="sidebar-toc" class="sidebar-toc">
      <div class="sidebar-toc-content">
        <div class="sidebar-toc-header">Contents</div>
        <div class="sidebar-toc-links">
          <a href="#A">Introduction</a>
          <a href="#B">&nbsp;&nbsp;Key Components</a>
          <a href="#C">&nbsp;&nbsp;Architecture</a>
          <a href="#D">Examples</a>
          <a href="#E">Challenges</a>
          <a href="#F">Conclusion</a>
        </div>
      </div>
    </div>

    <canvas id="gameOfLife" class="game-of-life" width="200" height="400"></canvas>
    <script src="functionality.js"></script>
  </body>
</html>
