<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>bb.radz</title>
    <link rel="icon" type="image/x-icon" href="../pics/pattern.ico" />
    <link rel="stylesheet" href="article-styles.css" />
    <link
      href="https://fonts.googleapis.com/css2?family=Noto+Sans:wght@400;700&display=swap"
      rel="stylesheet"
    />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
  </head>
  <body>
    <header>
      <div class="header-content">
        <h1>
          <a class="landingpage-button" href="../index.html">Ben Bradley</a>
        </h1>
        <div class="header-horizontal">
          <div class="left-horizontal">
            <p>Riding the Wave of Computation</p>
            <!-- <a href="/about"><p>About</p></a> -->
          </div>
          <div class="logo-horizontal">
            <a href="https://x.com/SLENDERdude441" target="_blank"
              ><p>
                <img
                  src="../pics/x6.png"
                  alt="x logo"
                  style="width: 15px; height: 14px"
                /></p
            ></a>
            <a href="https://github.com/bbradz" target="_blank"
              ><p>
                <img
                  src="../pics/git.png"
                  alt="github logo"
                  style="width: 15px; height: 15px"
                /></p
            ></a>
            <a href="https://www.linkedin.com/in/bbradz/" target="_blank"
              ><p>
                <img
                  src="../pics/linkedin.png"
                  alt="linkedin logo"
                  style="width: 15px; height: 15px"
                /></p
            ></a>
            <a
              href="https://open.spotify.com/user/slender_dude441"
              target="_blank"
              ><p>
                <img
                  src="../pics/spotify2.png"
                  alt="spotify logo"
                  style="width: 15px; height: 15px"
                /></p
            ></a>
            <a href="https://www.instagram.com/bbradz_/" target="_blank"
              ><p>
                <img
                  src="../pics/ig.jpeg"
                  alt="ig logo"
                  style="width: 17px; height: 15px"
                /></p
            ></a>
          </div>
        </div>
      </div>
    </header>

    <main>
        <h1 style="font-size: 32px; margin-bottom: 0px">
            From Jobs To Jensen
        </h1>
        <p style="margin-top: 5px; font-size: 12px">
            <strong>Word Count: 7,144</strong>
        </p>
        <div class="nav" style="font-size: 14px">
            <ol>
            <li><a href="#A">Introduction</a></li>
            <li><a href="#B">Motivation</a></li>
            <li><a href="#C">Baseline Physical Tradeoffs</a></li>
            <li><a href="#D">3D Parallelism</a></li>
            <ol>
                <li><a href="#E">Data parallelism</a></li>
                <li><a href="#F">Pipeline parallelism</a></li>
                <li><a href="#G">Model parallelism</a></li>
            </ol>
            <li><a href="#H">What even is a Parameter?</a></li>
            <li><a href="#I">ZeRO-- Seriously Tackling Where 3D Parallelism Fails</a></li>
            <ol>
                <li><a href="#J">Off-GPU, Intra-Node</a></li>
                <li><a href="#K">Further Miscellaneous ZeRO</a></li>
            </ol>
            <li><a href="#L">Conclusion</a></li>
            <li><a href="#M">Sources</a></li>
        </div>

        <hr />

        <a name="A"></a>
        <h3>Introduction</h3>

        <p>
            The joy of working in and following Tech is that it's constantly changing,
            what worked a decade ago is rapidly outclassed by new and innovative techniques
            which inspire everyone to remain constant learners and rise to every new moment. 
        </p>
        <p>
            From C to ChatGPT, Assembly to A Million 
            <a href="https://www.independent.co.uk/news/science/apollo-11-moon-landing-mobile-phones-smartphone-iphone-a8988351.html">
            Apollo's</a> in every pocket, the face of modern computation is defined 
            by decade-by-decade, year-by-year, and even month-by-month change. 
            What about today? What's the evolution driving the reinvention of Tech even
            as I type these words? It's difficult not to argue: The Era of the Hyperscalers. 
            
            Seven giants congregated around immense moats of network effects, wells of proprietary data, CAPEX 
            equivalent to G7 economies, and little helpings of 
            <a href="https://www.theverge.com/2024/8/5/24155520/judge-rules-on-us-doj-v-google-antitrust-search-suit">
            incest</a> have concentrated (to exagerate only slightly) the whole world's capital 
            under seven corporate projects with the remaining economy left stagnant 
            and hanging on for the ride.
        </p>
        <div class="centered-item-holder">
            <img
            src="../pics/Pic 0.webp"
            alt=""
            class="responsive-image-large"
            />
        </div>
        <p>
            While the impact of computation's reinvention of work continues to provide 
            a comet of growth streaking across an economy otherwise stagnant one can't 
            help but wonder how the food chain of value became so topsy-turvy. How 
            and when exactly did we move away from a world where the peak of The 
            Valley was Apple's compressions of all the world's information into our
            pockets and return to a IBM-style centralization of compute power so 
            swift that it renders even the 
            <a href="https://www.washingtonpost.com/business/2024/03/07/ai-data-centers-power/">
            electric grid</a> itself dwarfed. 
            When did we move from the era of Jobs to that of Jensen?
        </p>
        <p>
            Spoiler alert, I can't answer that question-- but <i>maybe</i> I can provide a
            helpful angle of context in considering it! For today's article I'll look 
            into one of the driving factors behind our modern era of Hyperscalers where trillion dollar 
            companies are minted in the mad rush to cluster as many NVIDIA GPUs under one roof as 
            feasible: the techniques behind scaling model training to distributed Data Center 
            Scale computing. 
        </p>

        <a name="B"></a>
        <h3>Motivation</h3>

        <p>
            In the early 2020s the literature around model performance began to shift when researchers
            at firms like Google and OpenAI (where the executives seemed to catch on the quickest) 
            began to discover the now everpresent 
            <a href="https://arxiv.org/pdf/2001.08361"><b>Model Scaling Laws</b></a> (Kaplan et al. 2020). 
            Deep Learning had been picking up steam since 2012 when AlexNet topped ImageNet 
            and proved deep models able to displace expert systems on complex image recognition 
            tasks but the real spark was the discovery of these "Scaling Laws for Neural 
            Language Models". Slowly, those in the know began to realize with startling precision 
            and predictability how the error rates of language models drops smoothly 
            with every logarithmic scaling of: A. model size, B. dataset size, and C. compute sunk.
        </p>
        <div class="centered-item-holder">
            <img
            src="../pics/Pic 16.png"
            alt=""
            class="responsive-image-med"
            />
        </div>
        <p>
            Since that paper internets worth of data, countries worth of compute, and mountainously 
            large models have bloomed under the light of 100s of billions worth of CAPEX. Everyone 
            in Tech has raced to dump dollars into a dynamic of model performance growth which seems 
            to leave whoever has the most resources for driving into training with a winner-take-all 
            trend in performance. A snowball of CAPEX which everyone who's anyone in the world of 
            tech could justify shovelling more and more cash into. 
        </p>
        <p>
            We'll focus particularly on that ballooing in model size since the 
            cutting-edge Large Language Models of today contain
            <a href="https://www.constellationr.com/blog-news/insights/meta-launches-llama-31-450b-and-zuckerberg-its-personal">
            450 billion</a> and 
            <a href="https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/">
            1.7 trillion</a> parameters roughly between Meta and OpenAI. The issue? 
            Given the standard FP32 datatype for each parameter (aka 4 bytes of memory per weight)
            that translates into some 14.4 to 54.4TB for storing model parameters alone! 
            Additionally, in the process of training a model each parameter ends up with multiple 
            additional values tied to it and in need of being stored. Necessary for running the cutting 
            edge optimizers which drive Trillion dollar performance is a explosion of memory footprint 
            by entire multiples of the already ballooning model parameter count, causing training 
            at it's core to approach up to 100TB of memory required for the model of today.
            The catch of course is that NVIDIAs largest GPUs max out at 80GB of on-device 
            memory, almost three order of magnitude below how much modern models require. 
            There isn't a self-contained GPU on Earth capable of training a ChatGPT 
            or LLama3 on it's own but by popular demand we continue to forge these models at bigger 
            and bigger scales, the delta-- we've figured out new and evolving ways to connect 
            together multiple computing platforms into unified platforms for distributed 
            training and drive the cutting edge of computation off of individual chips 
            into warehouses of 1,000s of chips. 
        </p>
        
        <a name="C"></a>
        <h3>Baseline Physical Tradeoffs</h3>
        <p>
            If your anything like me this is probably about the time when your brain fades away and 
            loses interest because <i>~HEY~ I'm into AI for the mental models, for abstracting problem solving, 
            not for the
            <a href="https://www.youtube.com/@Asianometry">photolithography </a> or
            <a href="https://en.wikipedia.org/wiki/InfiniBand">network topologies</a>, that's for the 
            Hardware guys</i>. I understand the feeling. Not only is it probably 
            good for you to brush up on the Hardware game from time to time but the physical dynamics 
            of the split systems these models are computing on really does matter for the code-level
            implementation of training these models. Silicon informs everything, fortunately there 
            are some relatively quick to cover baseline dynamics defining the challenge.
        </p>
        <p>
            The core of these distributed model training runs is a hierarchy of systems within systems 
            which we can simplify by starting at the unit of a single V100 GPU and zooming out. 
            GPUs are the smallest units as they individually have 1 GPU worth of memory
            (~32GB) but have higher internal data transfer speeds of roughly 900GB/s. Going 
            up a unit your looking at the DGX-2 rig which holds 16 V100s aka 512GB of GPU memory alongisde 
            1.5TB of system memory but has a data transfer rate between it's internal V100s of roughly 
            300GB/s. Finally we rise the highest unit: connecting together multiple DGX-2 rigs as 
            one where the memory scales up to the 100TB levels of Trillion dollar Hyperscalers but 
            where the transfer speed of data between rigs comes down to a mere 100GB/s. These 
            numbers 
            <a href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/dgx-1/dgx-2-datasheet-us-nvidia-955420-r2-web-new.pdf">
            [1]</a>
            change and if I were writing this article in 4 years then NVIDIA would be multiple
            generations of exponential improvement ahead on all these numbers but the point is 
            the physics of data transfer and system size, combined with the cost of scaling 
            up cutting edge connection speeds and scaling down huge amounts of memory define 
            the core challenges of distributed training. The game is understanding the resources 
            available at each unit-scale, understanding the benefits and tradeoffs of utilizing 
            higher unit-scales, and cleverly structuring of every phase of training to get as 
            many of the positives of scale while avoiding it's downsides. 
        </p>
        <div class="centered-item-holder">
            <img
            src="../pics/Pic 17.png"
            alt=""
            class="responsive-image-large"
            />
        </div>
        <p>
            <i>This</i> is why NVIDIA has catapulted to trillions in value, why Google's homegrown
            TPU computing platform is so lucrative, and why the rest of the Hyperscalers are hoovering 
            up any top-of-class GPU they can find to slot into their warehouses. Not only is 
            there immense pressure to crush more and more performance into less and less space 
            but there's also immense pressure to figure out how to scale the solutions currently 
            so expensive that they can only be used at small scales to those warehouse scales 
            where they bring down some of the costs of trying to plug a bunch of GPUs together.
        </p>
        <p>
            But while buying more GPUs provides you the gunpowder necessary for 
            GPT-Tier training it doesn't get you there by default, you need a technique for 
            actually spreading the training across all those GPUs and communicating between 
            them. You need a technique aware of the structure of the system it's being 
            trained on. For this we turn to the meat of this article: the big box of 
            parallel training techniques. 
        </p>

        <a name="D"></a>
        <h3>3D Parallelism</h3>

        <p>
            There are three fundamental techniques for parallel training which together 
            come together under the tile of <b>3D Parallelism</b> as the distinguishing
            dimensions along which to divy up work on a parallellizable set of computing
            platform. We'll tackel them in increasing order of complexity:
        </p>

        <a name="E"></a>
        <h4>Data parallelism</h4>

        <p>
            Data parallelism is when you have \(N \) devices to do computing on so you 
            copy-paste your model across across every device, split the data you plan 
            to train on into \(N \) batches, and then on each device train the full model 
            on it's own assigned batch of the data. 
        </p>
        <p>
            There's one catch however, a catch inherent to model training which eats 
            distributed training inherently comes against on top of the existing game of 
            maximizing hardware utilization: Pooling operations.
        </p>
        <p>
            Training a simple one layer Neural Network on even just 1 GPU has two
            fundamental steps, the Forward pass during which our model takes in data 
            and passes the output of the \(i \)th layer as input to the \(i+1 \)th layer up 
            until the final layer when the output of the whole model is produced and the 
            Backward pass during which the gradient of the error of our model's Forward pass 
            output is passed backward through chain rule to update every layer up until the 
            first one where the original input came in. The rub being that in order to 
            calculate the true gradient for propagation in the Backward pass you need to 
            pool together the outputs from every device to derive the average update implied 
            by the error on each sub-batch of data. Luckily GPUs are relatively fast at this
            but it does require shipping the outputs from every device through a operation 
            called <i>All-Reduce</i> to one of our GPUs which can then average them up into 
            one final gradient and ship them back to all of the devices through a <i>Broadcast</i>
            operation. In this way Data parallelism definitely speeds up training (barring 
            some extremely poor data transfer speed between devices) as it allows you to reduce
            the scale of computation & memory consumption per device down to \(\mathcal{O}(M) \) 
            from \(\mathcal{O}(DM) \) where \(M \) is the number of parameters and \(D \) is 
            the number of datapoints to compute across but it doesn't get the full job done.
        </p>
        <p>
            If you've been paying attention your probably a little confused right now
            however because the primary issue today is really that \(M \) term 
            not fitting onto a single device, not the \(D \) term. 
        </p>
        <div class="centered-item-holder">
            <img
            src="../pics/Pic 18.png"
            alt=""
            class="responsive-image-med"
            />
        </div>
        <p>
            While Data Parallelism is absolutely a dimension to split work along which 
            we'll return to later to bring training throughput up at all training scales
            once we have the model size problem pinned down it doesn't address the 
            core issue, our inability to fit the whole model onto one device. To 
            tackle this harder problem of distributed training we look to the two 
            other dimensions of distribution techniques which provide options for 
            splitting the model itself across devices.
        </p>

        <a name="F"></a>
        <h4>Pipeline parallelism</h4>
        
        <p>
            The clearest path for splitting up model execution across devices is to split 
            the layers of our model off in groups and send each of those layers to their own
            devices. Given N devices we could cleaning take a model with N layers and assign 
            each layer to it's own device. 
        </p>
        <p>
            Next we return to the trust Forward & Backward passes, fundamental operations of 
            training Neural Network models of any size. In order to pull off our Forward pass 
            we would need to input into the device containing the earliest layer block of our model
            our dataset to be trained on then once that device produces an output, ship that 
            data over to the device for layer block 2, then layer block 3, until our final layer outputs 
            (this time helpfully already on one device) the full error on our dataset which 
            can then on-device be turned into a gradient before our backward pass tracks it's
            way across our devices backwards for that Backwards pass of parameter updating. 
        </p>
        <p>
            This technique is called <b>Pipeline Parallelism</b> and while it avoids the cost
            of needing to gather outputs across devices for gradient calculation it also exposes
            us to the slower inter-device transfer speeds between each block of layers. 
        </p>
        <p>
            Another downside of Pipeline Parallelism is that, for example, the device of ours
            with the earliest layer block is under this naive implementation left idle for 
            the whole time from passing it's output to the second device at the start of our 
            Forward pass all the way to when it receives back the last stage of the Backward 
            pass. When each of our devices are many thousands of dollars cutting edge GPUs in 
            global short supply this translates into a pretty substantial amount of idle time 
            across all devices which it would really be better to avoid.
        </p>
        <p>
            These pockets of idle devices are termed "Bubbles" in our execution and thankfully 
            Pipeline Parallelism does have a bit of a solution to this in the form of <i>
            Micro-batching</i>. Microbatching operates by once again splitting our full dataset
            into \(N \) batches of data then after we calculate the result of our first layer 
            block on our first batch of data we still have \(N-1 \) batches of data to continue 
            running our layer blocks on which keeps them up and active running at overlapped times,
            unlocking a Bubble ratio of \(\approx \frac{N-1}{M+N-1} \), the following is a rough
            diagram of the process:
        </p>
        <div class="centered-item-holder">
            <img
            src="../pics/Pic 1.png"
            alt=""
            class="responsive-image-med"
            />
        </div>
        <p>
            Empirically this method of Micro-batching is found to work best when given \(M \)
            microbatches across \(K \) devices: \(M \geq 4K \), leading to near-linear scaling
            of throughput. 
        </p>
        <p>
            There's a broad class of approaches to reducing this Bubble ratio even further though with
            the primary approach I've come across being WPipe from 
            <a href="https://openreview.net/pdf?id=cw-EmNq5zfD">Yang et al. 2022</a> 
            which brings the Bubble Ratio down to zero at the expense of higher memory 
            requirements at any given time across the sum of the devices:
        </p>
        <div class="centered-item-holder">
            <img
            src="../pics/Pic 2.png"
            alt=""
            class="responsive-image-large"
            />
        </div>
        <p>
            If the central insight of Data parallelism is that you can split up the Data to 
            be ran through in parallel on seperate devices then the central insight of Pipeline
            parallelism is that you can split up a model horizontally into sequentially running 
            blocks which can be, through clever spacing, be ran in near parallel but in many 
            ways that clever spacing is only necessary because models are inherently sequential 
            algorithms temporally across their Forward & Backward passes. This is the observation 
            which Model parallelism builds on to introduce the thrid dimension of 3D parallelism 
            for splitting up model training, no longer splitting our models horizontally but 
            instead splitting them vertically.
        </p>

        <a name="G"></a>
        <h4>Model parallelism</h4>

        <p>
            Splitting models vertically means making our cuts at the intralayer level a cut which 
            is in many ways awkward and in some ways opens up it's own front for model design, 
            that being avoiding designs which require pooling. You see, much like in the case 
            of Data Parallelism where we needed to ship information from every device to one 
            central GPU for computing the average gradient across data batches Model parallelism 
            not only requires that same pooling of average gradients across each vertical slice 
            of our model but also, depending on model architecture, requires that pooling of 
            data to one GPU for every layer of our model which only operates when all of the 
            information in that layer is operated across. 
        </p>
        <p>
            Model Parallelism only first became feasible under the Megatron-LM algorithm 
            <a href="https://arxiv.org/pdf/1909.08053">(Shoeybi et al. 2020)</a> which outlined 
            methods for breaking up densely connected MLP layers and Transformer attention blocks
            into \(N \) vertical subslices with the minimal amount of necessary inter-GPU 
            expensive pooling operations:
        </p>
        <div class="centered-item-holder">
            <img
            src="../pics/Pic 3.png"
            alt=""
            class="responsive-image-med"
            />
        </div>
        <p>
            However, in many ways the fundamental challenge of Model Parallelism is working 
            around this inevitable cost of pooling which is sometimes the exact point of a 
            layer in a model (in the case of BatchNorm or Pooling layers) and often is 
            necessary for nonlinear functions (Ex: GeLU) which occur all across modern 
            architectures. As long as the Physics of data transfer make data transfer across 
            devices (especially at the largest unit sizes of Data Center scale training) this
            will be a primary issue which Model Parallelism runs up against.
        </p>
        <p>
            The other challenge of Model Parallelism which we've up until now mostly avoided 
            discussing is that in order to update the parameters of each layer in our Backward 
            pass we need to use the activation values of that layer as coming out of the chain 
            rule the activation value of a layer becomes a component of calculating the gradient 
            update for that same layer. One component of vertically slicing our models is that 
            per device the amount of activations being stored scales with the depth of the model 
            while the amount of actual computation being down at any given point of the Backward 
            & Forward pass reduces with the number of devices to send slices to. 
        </p>
        <p>
            Much like Microbatching helped in reducing the issue of Bubble Ratio for Pipeline 
            Parallelism however Model Parallelism's confrontation with activation value storage 
            can be reduced through trading off more computation for lower memory footprint in 
            a process called <i>Checkpointing</i>. With the knowledge that as we increase the 
            number of vertical slices we make in our model the actual computational needed at
            any given time to handle each slice decreases, translating to smaller MatMuls and 
            quicker execution, we can choose to store only every \(i \)th activation, resulting 
            in \(N \) stored activations where \(N = n_{\text{total_layers}}/i \) and then, 
            on our Backward pass, recalculate activations \((N-1)i\) to \(Ni\) from the 
            \((N-1)\)th checkpoint to only ever have \(N + i\) activations in memory at any 
            given time. Essentially this translates into recalculating activations from a 
            set of stores checkpoints of activations in order to increase the amount of computation
            in exchange for lower memory overhead of storing all of the activations for our 
            entire Forward & Backward passes. 
        </p>
        <p>
            Through careful Checkpointing and application of parrallization of non-pooling 
            layers Megatron-LM was determined to reach a new \(76\% \) scaling efficiency (meaning 
            only about \(24\% \) average idle time per GPU) on standard Large Language Models of 
            Transformers with equivalent & stable learning across device counts after only a 
            few minor reordering of some of the pooling steps. 
        </p>
        <p>
            That resolves all 3 primary axis of breaking down Model training into Distributed
            processes, importantly all of these techniques can be used in complement to eachother
            stacking huge performance gains and forming a greater family of distributed 
            computation referred to earlier as <b>3D Parallelism</b>. Given \(L \) vertical model 
            slices, \(M \) layer-level slices, and \(N \) data batches we arrive at space for
            \(L \times M \times N \) devices all running roughly in parallel (barring pooling
            and bubble ratios). 
        </p>
        <div class="centered-item-holder">
            <img
            src="../pics/Pic 4.png"
            alt=""
            class="responsive-image-med"
            />
        </div>
        <p>
            So we're all set then? We've found a way to split our model up along a whole 3 
            dimensions of device parallelism, surely that can scale enough to make GPT level 
            models possible! Well... not exactly. 
        </p>
        <p>
            You may have noticed that the memory requirements of a single DGX-2 node of 16 V100 
            GPUs still isn't large enough to hold an entire GPT level model training at once, 
            in order to get to the trillion parameter scales of 2024 Hyperscalers have moved to 
            operating at the  highest unit of Data Center scale, <i>inter-Node</i> training which 
            is far less efficient at a data transportation level than Intra-Node and 
            especially Intra-GPU data transfer.
        </p>
        <div class="centered-item-holder">
            <img
            src="../pics/Pic 8.png"
            alt=""
            class="responsive-image-med"
            />
        </div>
        <p>
            One of the faults with 3D Parallelism is that as we scale the number of devices we
            also scale the number of inter-device data transfers necessary for all of our pooling 
            layers and a few other portions of the model training process like those variables 
            references earlier produced by our optimizer which come in scales equal to some 
            16 times our original model parameter count. 3D Parallelism is a great building
            block for distributed training and an indispensable mental building block for
            for understanding the framework within which later frameworks for distirbuted training 
            are fitting their innovations around but it's not the end of the road. 
        </p>

        <a name="H"></a>
        <h3>What even is a Parameter?</h3>

        <p>
            Let's return to one of the sentences I laid out at the beginning of this article 
            in the back of the napkin calculations for model storage requirements section: 
        </p>
        <blockquote>
            <p>
            something something... storing all of those parameters in memory 
            in the historically standard FP32 format where each parameter takes up 32 
            bytes of memory translates to some 14.4TB to 54.4TB in memory...
            </p>
        </blockquote>
        <p>
            In this sentence lies one of the first clues for primary methods for pushing more
            model performance into less devices lowering the cost of training and lowering the 
            area of devices to transfer data between: Mixed Precision Training. The core insight 
            of Mixed Precision Training is that the model scale we're coming up against in our 
            need to distribute our model across devices in the first places is the bytes of 
            physical memory being required from our model and instead of lowering the total
            parameter count we can take aim at that traditional FP32 floating point precision
            (which takes up 4 bytes of memory) and drive the precision per parameter down. 
        </p>
        <p>
            But wait! Doesn't the precision of the values of those parameters matter? Isn't that 
            important information determining the performance of our model? Well yes, but there 
            are definitely a few tricks we can leverage to leverage less precise and more memory 
            efficient values for cheaper overall training. 
        </p>
        <p>
            Mixed Precision Training (Narang et al. 2018) introduced this idea through the
            general process of storing full-precision masterweights of our model but doing our 
            Forward (Fwd) & Backward (Bwd) passes in half-precision. While this algorithm 
            obviously has a cost in the most of a memory cost incurred by storing a full master 
            copy of our weights it pays off over time through lower memory overhead in every 
            corner of our training process. In reality, the majority of the overhead of memory 
            costs for training comes from the memory incurred by storing past activations (as 
            we confronted in the previous Model Parallelism section) so through adopting FP16 
            half-precision values for our entire Fwd & Bwd passes we reduce the vast majority 
            of the memory overhead we're actually worrying about. Not only does FP16 take only 
            half the memory space but since its lighter weight it's also quicker to operate on 
            which yields a computational / runtime improvement on top of that previously
            mentioned memory crisis. 
        </p>
        <p>
            As for that lost precision? Some of that is helped by storing full precision weights
            which our post-training end model is going to be stored in but another workaround 
            is through <i>Gradient Scaling</i>. One of the benefits of going to FP16 is that 
            in practice a huge amount of the gradient values throughout our Bwd pass fall into 
            the zero value under FP16 which opens up another surface of speeding up operations 
            through leveraging <i>sparsity</i> but thats a topic we'll reserve for the next
            algorithm we'll discuss, one of the downsides of many gradients in FP16 going to a 
            value of zero is that sometimes those gradients would have been a value which while 
            small was important for updating our weights and getting the full model to the
            accuracy these Hyperscalers needs. Thankfully there's a quick way of sorting out this 
            issue where through multiplying the pooled loss of our model after each Fwd pass 
            by a factor (\(\times 8 \) was used in the paper) we can bring some number of the highest value 
            gradients (an additional \(2\% \) of gradients in the paper) previously in the 
            zero range back into the non-zero range for FP16, restoring the gradients most 
            relevant for our training back into usability. 
        </p>
        <div class="centered-item-holder">
            <img
            src="../pics/Pic 5.png"
            alt=""
            class="responsive-image-med"
            />
        </div>
        <p>
            Of course not all of the Bwd & Fwd passes necessarily should be converted into FP16 
            and the authors of <i>Mixed Precision Training</i> make a point to highlight that 
            of the 3 categories of Neural Network arithmetic operations: Vector dot-products, 
            Reductions (aka Poolings), and Point-wise operations both Reductions & Vector
            dot-products seem to benefit form the precision of sticking with FP32. That being 
            said there are a variety of papers coming out in these past months (realize that 
            Mixed Precision is a 6 year old paper at this point) which have been showing further
            memory advances without sacrificing accuracy acheived over training such as
            <a href="https://arxiv.org/pdf/2402.17764"><i>The Era of 1-bit LLMs</i> (Ma et al. 2024)</a>
            and there has even been experimentation around reconfiguring how we allocate 
            bits within the 2 bytes taken up by a FP16 with Google having introduced the 
            <a href="https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus">
            BFloat16 data type</a> which, while taking up 2 bytes just the same as FP16, trades 
            off precision in value for a larger range of exponent, allowing for further 
            integration of those low value gradients without increasing memory overhead. 
        </p>
        <div class="centered-item-holder">
            <img
            src="../pics/Pic 6.png"
            alt=""
            class="responsive-image-med"
            />
        </div>
        <p>
            Mixed Precision Training and the field of re-examining what those parameters we're 
            working so hard to store are exactly / what we can strip back based on what we need
            from them is a very exciting dimension of the field of distributed training techniques.
            Reducing the weight of our weights is huge but all of that is to sidestep the issue 
            where scaling models to the size where they need to be distributed across grander 
            scaled units of interconnected GPUs leads to communication speeds which cause our 
            3D Parallelism's efficiency to crash. The core struggle which the next algorithm, 
            really a string of innovations out of Microsoft which has introduced a wide swath 
            of innovation into the game of distributed training, aims squarely at. 
        </p>

        <a name="I"></a>
        <h3>ZeRO-- Seriously Tackling Where 3D Parallelism Fails</h3>

        <p>
            ZeRO is a tremendously impactful project out of Microsoft's research team which  
            anyone involved in implementing these distributed training regimes would be bound 
            to come across thanks to incredibly helpful <i>Deepseek</i> library. Deepseek took
            off because it's a super simple interface built around the popular PyTorch ML library 
            but also because the team behind Deepseek introduced a huge amount of innovations 
            in distributed training techniques through ZeRO which made Deepseek one of the go 
            to libraries for distributed LLM traiining. For the sake of precision I'll avoid
            digging into the specifics of Deepseek's API and focus on those distributed techniques
            especially with the context of 3D Parallelism in mind. 
        </p>
        <p>
            The insighting issue in 3D Parallelism which ZeRO sought to bring more solutions 
            for is that on a Physical level as we bring more devices together, especially when 
            we need to bring multiple Nodes of devices together, the speed of communication
            across the whole system becomes worse and worse at every stage. ZeRO has become 
            a bit of a umbrella project under which are organized a huge amount of changes 
            to the way distributed training is done but at it's core ZeRO bring two primary 
            stepchanges: 
        </p>
        <p>
            <b>1.</b> Bringing down the memory overhead, allowing for more work to be done on each GPU
            and more bang behind each additional GPU.
        </p>
        <p>
            <b>2.</b> Clever utilization of CPU & Node-level memory / computation to pass off training 
            requirements between close-by components of these DGX-2 Nodes of many GPUs, allowing 
            important memory to be kept closer to the GPU and shipped off between Nodes less often.
        </p>
        <p>
            One of the fundamental axis' of 3D Parallelism is the vertical split aka Model 
            Parallelism (MP), while MP works well within a single node where inter-GPU bandwidth 
            is more efficient, that efficiency quckly degrades once the scale of the system being 
            trained reaches beyond a single node. Empirical tests from Microsoft regarding 
            training a 40 billion parameter model, a model size which requires expanding training 
            into 2 DGX-2 nodes, found that hardware utilization fell to a miniscule 5% of peak 
            capacity. Looking closer at training however there's more than just parameters being 
            kept on each GPU during training and while we've talked about lowering the memory cost
            of parameters and talked about techniques like checkpointing to lower the amount of 
            layer activations to store for our Bwd pass in practice the majority of memory is 
            usually taken by the states of our optimizer!
        </p>
        <p>
            If you feel like you may want to brush up on Optimizers now is the time for me 
            to highlight that I actually just wrote a 3-part series on the field of optimizer 
            types, benchmarking optimizers, and the newest glitzy optimizer popping up today. 
            Suffice to say if you haven't read those article though, in order to translate the 
            error of our model into the exact amount to adjust each model parameter by it's 
            been empirically (& theoretically) found that keeping optimizer states stored in memory 
            for the sake of tracking trends in how training is progressing can be invaluable 
            for getting the most high performance model after training as possible, unfortunately
            that means a whole other set of values multiple times larger than even the parameter 
            count needing to be stored during training.
        </p>
        <p>
            ZeRO stands for Zero Redundancy Optimizer because while 3D Parallelism looks at the
            parameter values and even the activation values it hasn't been known to address these 
            optimizer values, leaving a whole other block of redundant values being stored on 
            GPUs which can be split up across devices. Additionally, whereas gradients & 
            parameters had at the time of ZeRO been shown to be able to be compressed into FP16
            (this was a large thrust of the takeaway of Mixed Precision Training), because
            smaller changes in optimizer values have reprecussions which echo through the entire 
            rest of the training process they still had to be kept in FP32, meaning that not 
            only are there more optimizer values than parameter values but that each optimizer 
            value takes up twice the memory of each parameter / gradient value! 
        </p>
        <div class="centered-item-holder">
            <img
            src="../pics/Pic 7.png"
            alt=""
            class="responsive-image-med"
            />
        </div>
        <p>
            ZeRO defines 3 stages of parallelizing these various redundant values categories:
        </p>
        <p>
            <b>1. \(P_{\text{os}}\)</b> -- Partitioning <i>optimizer</i> states across GPUs in a way similar to traditional 
            MP, yielding a \(\times 4 \) memory reduction with the same inter-GPU communication 
            overhead.
        </p>
        <p>
            <b>2. \(P_{\text{os+g}}\)</b> -- Partitioning <i>gradient</i> states across GPUs bringing a \(\times 8 \) memory 
            reduction again with the same inter-GPU communication, and, 
        </p>
        <p>
            <b>3.\(P_{\text{os+g+p}}\)</b> -- Partitioning <i>model</i> parameters, yielding a memory reduction which scales 
            linearly with the number of GPUs trained on and a 50% increase in inter-GPU 
            communication. 
        </p>
        <p>
            Using all 3 stages of redundant value partitioning, ZeRO unlocks the ability to 
            train a 1T parameter model (aka 16TB of memory) on only 1024 NVIDIA GPUs (16GB per GPU
            half of the memory limits of most industrial-use GPUs today).
        </p>
        <p>
            That alone brings ZeRO much of the way to widespread industrial application but not 
            the whole way, while just partitioning our values across all 1,024 GPUs would allow 
            for training a 1T parameter model the ZeRO team shows that the training run would 
            end out taking more than a year! Not only is that a huge amount of time to get a 
            product trained if your trying to get a product like ChatGPT (or it's competitors)
            to market but it also restricts capabilities for researchers looking to execute 
            large-scale experiments of the kind necessary to keep pushing LLM architecture 
            forward. 
        </p>
        <p>
            Thus ZeRO brings another suite of innovations as well--
        </p>
        <p>
            To start, one can allot buffers of memory on each GPU and micromanage the specifics 
            of the physical distribution of each of the tensors written in to memory to 
            pre-clear and adaptively reconfigure our memory, keeping memory chunks as contiguous 
            and as efficiently distributed as possible. 
        </p>
        <p>
            I could dig further into the details here around both 
            these components of intelligent memory allocation but suffice to say ZeRO brought 
            to the front of public conversation new algorithms for automatically setting up 
            contiguous chunks of memory buffer based the specifics of both the model being 
            trained & the system being trained on, setting the stage for better memory allotment 
            during training. Then during training in order to keep that distribution as healthy
            as possible, as contiguous and efficiently distributed as possible, one can define
            a relatively simple protocol of tensor age tracking to proactively utilize any time 
            where chunks of data aren't being utilized for the current phase of the Fwd/Bwd 
            passes to move around (or even delete) data and in the process eliminate unusable
            fragmented memory index while lowering the memory footprint and making it quicker
            and lighter to operate on that memory when the Fwd/Bwd pass eventually comes back 
            to reuse that memory.
        </p>
        <p>
            After that we turn to yet another set of values which can be explicitly set to 
            be partitioned across devices: <i>activation</i> values. We've discussed 
            <i>checkpointing</i> as a technique for lowering the amount of layer activation 
            values stored for weight adjustment in our backward pass but utilizing this idea 
            of partitioning memory to only the devices where it's going to be directly used 
            we can partition those activation values, enhancing memory savings even on top of 
            that checkpointing rate. 
        </p>
        <p>
            So-- we've partitioned all of these parameters, gradients, optimizer states, and 
            activation values across GPUs to the models where they actually matter. Then we 
            put in place some protocols for automatically setting up the memory requirements 
            that our model is going to need to train on the physical computing network we're 
            working with, maintaining a healthy contiguous distribution of saved memory blocks
            which lowers memory requirements even further. 
        </p>
        <p>
            We're doing an awful lot of work in order to fit all of these values onto our 
            GPUs and it's useful to return to asking that all important question, why? Well,
            because we need our saved states in memory close by to where their going to be 
            computed on since the further we store our states from the location of computation
            the longer and less efficient it gets to communicate that data over to load into 
            that location of computation, and the location of computation is the GPU... right?
        </p>
        <p>
            Enter the CPU and enter NVMe memory. 
        </p>

        <a name="J"></a>
        <h4>Off-GPU, Intra-Node</h4>

        <p>
            GPUs are incredibly efficient for the multidimensional operations which dominate 
            the compute cost of training LLMs but DGX-2 Nodes also have within them the top of
            the line CPUs and NVIDIA's next-generation of SSD storage called NVMe. While 
            computation takes much longer on the CPU than GPU because GPUs are just so efficient
            looking at a memory breakdown of where the memory on a DGX-2 Node is placed we 
            see that CPUs have twice the memory capacity of our GPUs and our NVMe drives have 
            \(\times 14 \) the memory of both the CPUs and GPUs combined. Because of this, 
            despite the costs of busing data from GPU to CPU or from GPU to NVMe storage, there's 
            real worth in offloading data into these (comparatively) bulk stores of data 
            and even running compute on the CPU whenever we can justify it not bottlenecking 
            access of our more efficient GPUs to the information they need. 
        </p>
        <div class="centered-item-holder">
            <img
            src="../pics/Pic 9.png"
            alt=""
            class="responsive-image-med"
            />
        </div>
        <p>
            While the Bwd & Fwd passes compute surface scales with the product of \(N \) our 
            parameter count and dataset size \(D \), some of the operations happening in our 
            model scale with parameter count \(N \) alone and those are the types of operations
            which CPUs can compete with GPUs on and therefore can be offloaded to the CPU during 
            our training process. These would be norm calculations necessary for weight 
            regularization or weight updates which consider every weight once and only once. 
        </p>
        <p>
            In fact, we can offload some of those partitioned model states (essentially 
            everything except our parameters) onto those CPUs both to find a way for storing off-GPU 
            but not <i>too</i> off-GPU, as in the case of activations, or to slowly chip away 
            at minor portions of those compute volume which our GPUs are dramatically more 
            efficient on but which our CPUs have enough idle time to bear some of the load 
            on.
        </p>
        <p>
            After a bit of smart compute-communication overlap management we can then
            fully incorporate leveraging what off-GPU storage (and even computation) we have 
            on our CPUs and NVMe SSDs while avoiding incurring the steep communication speed 
            dropoff you get from moving memory between DGX-2 Nodes. 
        </p>
        <div class="centered-item-holder">
            <img
            src="../pics/Pic 10.png"
            alt=""
            class="responsive-image-med"
            />
        </div>

        <a name="K"></a>
        <h4>Further Miscellaneous ZeRO</h4>

        <p>
            That's the bulk of what makes ZeRO ZeRO, how redundancies get optimized out through
            bringing all of the resources locally available to bare on supporting training 
            as fully as possible. There are a bunch other breakthroughs which I would be 
            remise to mention but which in my reading generally begin to diverge from being 
            fundamentally rooted distributed computation and move closer towards just useful 
            techniques for training Transformers. 
        </p>
        <p>
            Those are as follows: 
        </p>
        <p><b>1. Quantization & Hierarchical Partitioning of Weights
        </b></p>
        <p>
            One of the bottlenecks of the Fwd & Bwd pass process is in the gathering of 
            weights across all of the GPUs to calculate average error and produce an average
            gradient for our Bwd pass since it requires a gathering of values across every 
            device. One technique for delivering the messages from each device to the central
            node operating on those gathered values without each message taking up quite as 
            many bits of communication is through a method called quantization which amounts 
            to mapping our high precision values into a smaller lower precision range then 
            mapping them back up to the high precision range at their destination with a small
            error of information lost in compression. In a extension of ZeRO called 
            <a href="(https://arxiv.org/pdf/2306.10209)">ZeRO++</a>
            the team at Microsoft observed that by quantizing a block of values according to 
            seperate mapping functions for each column block they could reduce that error 
            from quantization by \(\times 3 \) without increasing the bits being transferred 
            between machines.
        </p>
        <div class="centered-item-holder">
            <img
            src="../pics/Pic 11.png"
            alt=""
            class="responsive-image-med"
            />
        </div>
        <p>
            ZeRO++ also introduces a technique for reordering data inside of each DGX-2 
            Node in order to structure the feed of information being sent between nodes so 
            as there is less communication than under more naive methods. They did this 
            through reordering of tensor slices inside of each node to correct gradient
            misplacements which otherwise would propogate across nodes.
        </p>
        <div class="centered-item-holder">
            <img
            src="../pics/Pic 12.png"
            alt=""
            class="responsive-image-med"
            />
        </div>
        <p>
            Finally through gathering pooled weights in multi-server sub-clusters of 
            the full training system during the Fwd & Bwd passes the ZeRO++ team found 
            the ability to tradeoff \(\times 8.9 \) memory cost for \(\times 1.5 \) end-to-end 
            communication on those all-device data pooling operations. 
        </p>
        <div class="centered-item-holder">
            <img
            src="../pics/Pic 13.png"
            alt=""
            class="responsive-image-med"
            />
        </div>
        <p>
            By combining all 3 of these techniques ZeRO++ was found to achieve nearly 
            \(45\% \) sustained throughput compared to the theoretical hardware peak, a 
            consistent \(\times 2.4 \) speedup in time-to-train for a mock training run of
            GPT-3 over ZeRO alone, and similar throughput to ZeRO even on clusters with 
            \(1/4 \) the throughput capability. 
        </p>
        <p><b>2. Customized Kernels adapted to leverage Sparse Attention</b></p>
        <p>
            The defining mechanism of Transformers as an architecture which led to their 
            dominance and much of the performance behind modern LLMs is that through an 
            Attention mechanism they learn the association between every word in their input 
            sequence and every other word in their input sequence, generating a \(\mathcal{O}(n^2) \)
            memory requirement for storing the full learned Attention structure. Fortunately, 
            only a few of the connections in that Attention mechanism are likely to be strong 
            within that grid and the vast majority of connections take on a value of essentially 
            zero.
        </p>
        <div class="centered-item-holder">
            <img
            src="../pics/Pic 14.png"
            alt=""
            class="responsive-image-med"
            />
        </div>
        <p>
            This is a dynamic known as sparsity and through writing up careful GPU-level 
            kernels one can choose to not operate on the full grid but instead only on some 
            subset of the grid which they find to contain as many of those strong (and as few zero) 
            signals as possible. Through a custom sparse attention kernel LLM training 
            can be scaled to \(\times 10 \) longer sequence lengths and \(\times 6 \)
            execution speeds without jeopardizing model error rate.
        </p>
        <p><b>3. 1-bit Adam </b></p>
        <p>
            Much like in the case of Weight Quantization, one way of saving communication 
            costs on sending optimizer states between devices is to compress the information 
            from those optimizer states during transfer. Unfortunately, the most efficient 
            data compression algorithms only work on optimizers where the update rule is 
            linearly dependent on the gradients while the high detailed weight updates which 
            lead to the highest model accuracy after training come from optimizers like Adam 
            which introduce factors beyond that linear causation. However, since empirical 
            findings show that the non-linear component of Adam (aka it's variance term) 
            begins to stabilize to a relatively constant value early on into training one can 
            subject themselves to the slower communication costs for a short warmup period in 
            the beginning of training then save that variance value which their model 
            converged to and switch to a linear optimizer for the rest of training corrected 
            by that saved variance term. Therefore weight compression can be utilized for 
            the majority of training and the memory overhead of storing all of the optimizer 
            states can be brought down, resulting in communication volume reducing by up to 
            \(\times 5 \) without a drop in convergence efficiency.
        </p>
        <p>
            ZeRO is a project title, not a technique in and of itself, but the techniques which 
            the team behind it at Microsoft Research highlight treats seriously the shortfalls 
            of approaching distributed training through merely the lens of 3D Parallelism and 
            introduces new topics like smart value partitioning, protocols for maintaining reliably
            contiguous memory, moving memory and computation off of our GPUs while keeping them 
            on-Node by offloading onto the CPU and NVMe SSDs, as well as many many quantization, 
            kernel, and optimizer tricks. The list goes on. 
        </p>
        <p>
            At it's core ZeRO seeks out ways to decrease issues from bottlenecks around the 
            on-device memory of all of our GPUs and in the process free up every GPU to train on 
            higher batch sizes, pushing throughput of model training to the point of scaling 
            at superlinear rate compared to the number of GPUs being trained on. 
        </p>
        <div class="centered-item-holder">
            <img
            src="../pics/Pic 15.png"
            alt=""
            class="responsive-image-large"
            />
        </div>

        <a name="L"></a>
        <h3>Conclusion</h3>
        <p>
            They say "may you live in interesting time" and the beautiful thing is that as 
            long as you lock in on the innovations happening in the Tech industry, there's 
            no time more interesting than today. Between trillions in value, the national 
            strategies of Superpowers, and being my field of study the whole world is turning 
            on a knifes edge around the Winner-Take-All race to win the race for AI dominance
            in a world where, atleast when it comes to foundation models, bigger equals better. 
        </p>
        <p>
            No one knows who's going to win that race, no one knows if these foundation models 
            are going to find the economic use-cases to justify their eye-watering pricetags, 
            and no one knows if the scaling laws driving this ballooning of models will continue 
            to drive the nature of cutting edge model work further and further into reliance 
            the Distributed Training techniques outlined above in this article. What everyone 
            knows is that we're pushing machines to being able to do things previously thought 
            squarely reserved to human minds and that the march of computation is, decade after 
            decade reshaping the face of American from the highest echelons of corporate 
            monopolies to the rectangle in our pockets.
        </p>
        <p>
            I'll do my best to continue soaking in the newest innovations, learning, and 
            doing what I find most rewarding in following, understanding, and projecting forward 
            the innovations being made around the world in computation. 
        </p>
        <p>
            To track our transition from the age of Jobs to that of Jensen. 
        </p>

        <a name="M"></a>
        <h3>Sources</h3>

        <ul>
            <li><a href="https://arxiv.org/pdf/2001.08361">Scaling Laws for Neural Language Models (Kaplan et al. 2020)</a></li>
            <li><a href="https://www.constellationr.com/blog-news/insights/meta-launches-llama-31-450b-and-zuckerberg-its-personal">Constellation Research: llama3.1</a></li>
            <li><a href="https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/">The Decoder: GPT-4 Architecture</a></li>
            <li><a href="https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/dgx-1/dgx-2-datasheet-us-nvidia-955420-r2-web-new.pdf">NVIDIA DGX-2 Datasheet</a></li>
            <li><a href="https://openreview.net/pdf?id=cw-EmNq5zfD">Group-based Interleaved Pipeline Parallelism for Large DNN Training (Yang et al. 2022)</a></li>
            <li><a href="https://arxiv.org/pdf/1909.08053">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism (Shoeybi et al. 2020)</a></li>
            <li><a href="https://arxiv.org/pdf/2402.17764">The Era of 1-bit LLMs (Ma et al. 2024)</a></li>
            <li><a href="https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus">BFloat16: The secret to high performance on Cloud TPUs</a></li>
            <li><a href="https://arxiv.org/pdf/1910.02054">ZeRO: Memory Optimizations Towards Training Trillion Parameter Models (Rajbhandari et al. 2020)</a></li>
            <li><a href="https://arxiv.org/pdf/2306.10209">ZeRO++: Extremely Efficient Collective Communication for Giant Model Training (Wang et al. 2023)</a></li>
            <li><a href="https://arxiv.org/pdf/2104.07857">ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning (Rajbhandari et al. 2021)</a></li>
            <li><a href="https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/">ZeRO-Infinity and DeepSpeed: Unlocking model scale for DL training</a></li>
            <li><a href="https://arxiv.org/pdf/2101.06840">ZeRO-Offload: Democratizing Billion-Scale Model Training (Ren et al. 2021)</a></li>
            <li><a href="https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/">DeepSpeed: Extreme-scale Model Training for Everyone</a></li>
            <li><a href="https://medium.com/@vishal09vns/sparse-attention-dad17691478c">Demystifying Sparse Attention: A Comprehensive Guide from Scratch</a></li>
            <li><a href="https://www.microsoft.com/en-us/research/blog/zero-2-deepspeed-shattering-barriers-of-deep-learning-speed-scale/">ZeRO-2 & DeepSpeed: Shattering <Btn></Btn>arriers of DL Speed & Scale</a></li>
            <li><a href="https://www.youtube.com/watch?v=zqsOEzKZX2Y">[YT] ZeRO & Fastest BERT: Increasing the Scale and Speed of Deep Learning Training in DeepSpeed</a></li>
        </ul>
    </main>

    <footer>
      <p>All opinions are my own and do not represent those of a employer.</p>
    </footer>
  </body>
</html>
