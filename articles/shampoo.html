<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>bb.radz</title>
    <link rel="icon" type="image/x-icon" href="../pics/pattern.ico" />
    <link rel="stylesheet" href="article-styles.css" />
    <link
      href="https://fonts.googleapis.com/css2?family=Noto+Sans:wght@400;700&display=swap"
      rel="stylesheet"
    />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
  </head>
  <body>
    <header>
        <div class="header-content">
            <h1><a class="landingpage-button" href="../index.html">Ben Bradley</a></h1>
            <div class="header-horizontal">
              <div class="left-horizontal"> 
                <p>Riding the Wave of Computation</p>
                <!-- <a href="/about"><p>About</p></a> -->
              </div>
              <div class="logo-horizontal">
                <a href="https://x.com/SLENDERdude441" target="_blank"><p><img src="../pics/x6.png" alt="x logo" style="width:15px;height:14px;"></p></a>
                <a href="https://github.com/bbradz" target="_blank"><p><img src="../pics/git.png" alt="github logo" style="width:15px;height:15px;"></p></a>
                <a href="https://www.linkedin.com/in/bbradz/" target="_blank"><p><img src="../pics/linkedin.png" alt="linkedin logo" style="width:15px;height:15px;"></p></a>
                <a href="https://open.spotify.com/user/slender_dude441" target="_blank"><p><img src="../pics/spotify2.png" alt="spotify logo" style="width:15px;height:15px;"></p></a>
                <a href="https://www.instagram.com/bbradz_/" target="_blank"><p><img src="../pics/ig.jpeg" alt="ig logo" style="width:17px;height:15px;"></p></a>
              </div>
            </div>
        </div>
    </header>

    <main>
        <h1 style="font-size: 32px; margin-bottom: 0px">
        Shampoo clears the competition
        </h1>
        <p style="margin-top: 5px; font-size: 12px">
            <strong>Word Count: 1,490</strong>
        </p>

        <div class="centered-item-holder">
            <img
            src="../pics/Screenshot 2024-08-19 at 6.31.10â€¯PM.png"
            alt=""
            class="responsive-image-large"
            />
        </div>

        <p>This is a continuation of my <a href="../articles/optimizers.html">Optimizers</a> article.</p>

        <p>As I noted in my optimizers article, the primary issue with second-order optimization 
        algorithms has been that while it's useful for efficient convergence to operate off 
        of the second-order Hessian of our surface: </p>

        <p><b>1. </b> Directly calculating (and even approximating) the Hessian is extremely 
        computationally expensive, and,</p>

        <p><b>2. </b> The Hessian definitionally stores multiple dimensions worth of the gradient 
        vector making it inherently more memory expensive than simply storing the gradient 
        alone. This presents design challenges of lowering the dimensions of the approximation 
        of the Hessian without sacrificing accuracy which Quasi-Newtonian optimizers have 
        sought to overcome.</p>

        <p>While the Hessian contains important information about the curvature of the 
        surface our model is optimizing along, which can dramatically improve
        convergence during training, the size of the Hessian scales quadratically with the 
        number of parameters in your model, eating up substantial amounts of compute, memory, 
        and time. Therefore the Hessian scales such that, in the era of deep NNs being 
        where the interesting performance gains are being found, any optimizer seeking to meaningfully 
        account for Second-Order information needs to find a way to navigate the majority of 
        the important information from the Hessian into a much smaller overhead.</p>

        <p>Shampoo, enhanced by a multi-GPU distributed implementation recently released by 
        Meta, is a particular attempt to work through this design challenge which has been drawing
        heavy attention in the recent weeks for having topped the <i>Algoperf</i> rankings
        and dethroning the usual Adam-Type algorithms which many of us interested in 
        following the field have grown to expect at the top of these types of well-rounded 
        optimizer benchmarks. This, combined with social media picking up on Shampoo as 
        having been the little recognized optimizer of choice for training Google's Ad
        recommendation pipeline <a href="https://arxiv.org/pdf/2209.05310">(Anil et al. 2022)</a>,
        has really super charged my questions about this pop-up 
        innovator in the field of optimizers. If Shampoo truly tops both the public & 
        private benchmarks of empirical and business applicability then how could I rest 
        on the laurels of my recent breakdown of optimizers without giving some light to this 
        high alpha fresh addition to the field? Thus I aspire to explain the mechanisms 
        and motivations behind the distributed Shampoo optimizer. </p>

        <p>The <b>core insight</b> behind the derivation of the Shampoo algorithm is the following:</p>

        <p>The Hessian matrix of a function measures how that function's output depends on each 
        possible combination of two of it's inputs:</p>

        \[ \mathbf{H}_f = \text{Hess} = \nabla^2f = \begin{bmatrix}
        \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1
        \partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1 \partial x_n} \\
        \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2
        f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2 \partial
        x_n} \\ \vdots & \vdots & \ddots & \vdots \\ \frac{\partial^2 f}{\partial
        x_n \partial x_1} & \frac{\partial^2 f}{\partial x_n \partial x_2} &
        \cdots & \frac{\partial^2 f}{\partial x_n^2} \end{bmatrix} \]

        <p>Alternatively, the observed Fisher matrix is a statistical object measuring how 
        much information each combination of two inputs from our function carries about the 
        value of our function: </p>

        \[
        \mathcal{J}(\theta^*) = -\nabla\nabla^{\top} \ell(\theta) \bigg|_{\theta = \theta^*} 
        = - 
        \left( 
        \begin{array}{cccc}
        \frac{\partial^2}{\partial \theta_1^2} & \frac{\partial^2}{\partial \theta_1 \partial \theta_2} & \cdots & \frac{\partial^2}{\partial \theta_1 \partial \theta_p} \\
        \frac{\partial^2}{\partial \theta_2 \partial \theta_1} & \frac{\partial^2}{\partial \theta_2^2} & \cdots & \frac{\partial^2}{\partial \theta_2 \partial \theta_p} \\
        \vdots & \vdots & \ddots & \vdots \\
        \frac{\partial^2}{\partial \theta_p \partial \theta_1} & \frac{\partial^2}{\partial \theta_p \partial \theta_2} & \cdots & \frac{\partial^2}{\partial \theta_p^2}
        \end{array} 
        \right) \ell(\theta) \bigg|_{\theta = \theta^*}
        \]

        <p>Notice any similarities? Good! Because the observed Fisher matrix is for all 
        intensive purposes essentially an approximation of the Hessian. This means through 
        the Fisher matrix there's a different perspective into second-order information and
        this is the path, an exciting new angle, which Shampoo takes towards approximating 
        our Hessian by utilizing methods for approximating the Fisher matrix instead of the 
        Hessian directly. </p>

        <p>Shampoo builds on
        <a href="https://arxiv.org/pdf/1503.05671">Kronecker-factored approximate Curvature (K-FAC)</a> 
        (Martens and Grosse, 2020) an efficient method for approximation of the Fisher information matrix 
        of a Neural Network through the Kronecker product of two smaller matrices. In this way Shampoo 
        brings the memory overhead down from quadratic to a constant factor of about 4-7 times parameter count, 
        moving Hessian utilization out of the realm of being prohibitively expensive and 
        squarely into practical applicability.</p>

        <p>The core innovation of Shampoo in relation to K-FAC is that instead of deriving our Fisher 
        matrix through directly sampling outputs of our model we can approximate both of those 
        smaller matrices through some pretty clever transformations of the first-order gradient
        alone.</p>

        <p>Shampoo approximates the Fisher matrix through maintaining two particularly memory-efficient 
        matrices: \(L, R \) which serve as running sums of distinct mappings of the gradient, 
        together preconditioning the rows and columns of our gradient matrix \(G_t \) at 
        each step.</p>

        <p>The algorithm for Shampoo is as follows:</p>

        \[
        \begin{gather}
            &\bar G_t = \alpha \bar G_{t-1} + (1-\alpha)G_t \\
            &L_t = L_{t-1} + \bar G_t \bar G_t^T \\
            &R_t = R_{t-1} + \bar G_t^T \bar G_t \\ \\ 
            &L_0 = \epsilon I \\
            &R_0 = \epsilon I \\
            &\overline{G}_0 = 0 \\ \\
            & \theta_{t+1} = \theta_t - \eta_t \overline{\mathbf{A}}_t^{-1/2} G_t
        \end{gather}
        \]

        <p>where \(\overline{\mathbf{A}}_t\) is a block diagonal matrix of the form</p>

        \[
        \begin{align}
            \overline{\mathbf{A}}_t 
            = \begin{bmatrix}
                \left[\mathbf{L}_t^{(1)}\right]^{1/2} \otimes \left[\mathbf{R}_t^{(1)}\right]^{1/2} & 0 & \cdots & 0 \\
                0 & \left[\mathbf{L}_t^{(2)}\right]^{1/2} \otimes \left[\mathbf{R}_t^{(2)}\right]^{1/2} & \cdots & 0 \\
                \vdots & \vdots & \ddots & \vdots \\
                0 & 0 & \cdots & \left[\mathbf{L}_t^{(n)}\right]^{1/2} \otimes \left[\mathbf{R}_t^{(n)}\right]^{1/2} \\
            \end{bmatrix}
        \end{align}
        \]

        <p>Put into maybe more direct (at the expense of obfuscating some details) terms, that 
        translates into the following simpler update rule 
        <a href="https://proceedings.mlr.press/v80/gupta18a/gupta18a.pdf">(Gupta et al. 2018)</a>
        :</p> 

        \[
        \begin{gather}
            &\overline{G}_t = \alpha \overline{G}_{t-1} + (1-\alpha)G_t \\
            &L_t = L_{t-1} + \overline{G}_t \overline{G}_t^T \\
            &R_t = R_{t-1} + \overline{G}_t^T \overline{G}_t \\ \\ 
            &L_0 = \epsilon I \\
            &R_0 = \epsilon I \\
            &\overline{G}_0 = 0 \\ \\
            &\theta_{t+1} = \theta_t - \eta L_t^{-1/4}\overline{G}_tR_t^{-1/4}
        \end{gather}
        \]

        <p>As you can see from the expansion of the \( \mathbf{\overline{A}}_t \) term, Shampoo 
        expands on a trend seen in other Hessian approximating optimizers through storing 
        column-transforming and row-transforming matrices which together distill the 
        information of a block-diagonal approximation of the Hessian. Where Shampoo differs 
        from past optimizers however is that it preserves some of the off-diagonal 
        information of the Hessian as well in the not exactly diagonal elements of it's 
        diagonal blocks, allowing for an eeking out a healthy helping of additional
        performance gains.</p>

        <p>In effect, by storing & utilizing it's \(L, R \) submatrices in the way it does
        Shampoo is able to store and compute an approximation of a full structured Kronecker 
        product preconditioner without explicitly calculating, storing, or operating on the 
        full structured matrix. </p>

        <p>In fact, Shampoo's update rule can be 
        <a href="https://arxiv.org/pdf/2406.17748">proven</a> (Morwani et al. 2024) to not only to preserve particularly 
        well the small eigenvalues of the full Kronecker matrix preconditioner which are 
        often thought to be the most important ones for effective preconditioning but work 
        out of Harvard shows that \(L \) and \( R \) upper bound the true Hessian by 
        approximating a Kronecker product equal to approximately the square root of the 
        optimal Kronecker approximation, a remarkably accurate approximation.</p>

        <p>The authors of Shampoo prove (through the body of their paper) that, with respect 
        to the number of iterations spent training \(T \) the bound on the regret (aka error) 
        of Shampoo scales by \(O(\sqrt{T}) \), provably the best possible bound for stochastic 
        optimizers. On top of that, through raising it's submatrices to the \(-1/4 \) as an exponent 
        Shampoo helpfully obtains a learning rate decay rate of \(O(1/\sqrt{t}) \) commonly 
        viewed as the ideal decary rate for stochastic optimization.</p>


        <div class="centered-item-holder">
            <blockquote class="twitter-tweet">
            <p lang="en" dir="ltr">So Shampoo has been getting some renewed attention for winning 
                one of the inaugural AlgoPerf challenges. I wanted to understand what the method 
                is doing, so I employed my favourite trick of just ~directly interpreting the 
                pseudocode~<br><br>(1/8) 
            <a href="https://t.co/0VlJRQ9rt6">pic.twitter.com/0VlJRQ9rt6</a>
            </p>&mdash; Jeremy Bernstein (@jxbz) 
            <a href="https://twitter.com/jxbz/status/1819846348130418706?ref_src=twsrc%5Etfw">August 3, 2024</a></blockquote> 
            <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
        </div>

        <p>Another little appreciated aspect of this algorithm I've observed is that
        \( L_0^{-1/4}G_0R_0^{-1/4} = \text{ortho}(G) \), geometrically meaning that at 
        the singular values (aka the magnitude in each vector direction which our weight matrix is 
        being scaled by) of the first step made by Shampoo are all snapped to a value of one. 
        This translates into minimizing divergence in weight values, cutting down on overfitting 
        and improving training. Outside of that exact first timestep, the update rule doesn't exactly snap 
        singular values to one but does do a sort of smoothed approximation of all the singular values 
        towards one accounting for sampling variance. This borrows in large part from a technique 
        called Spectral Normalization which has gained attention in GANs for controlling the 
        <a href="https://www.linkedin.com/pulse/understanding-lipschitz-constant-yeshwanth-n-gdplc">
            Lipschitz constant 
        </a> of the model's layers, a useful signal for encouraging better weight arrangments.</p> 

        <p>At it's core, Shampoo is all about picking the right pre-conditioner <i>(pun very much 
        intended by it's creators)</i>. It breaks down the memory expensive Hessian downstream of the
        high parameter counts dominating the modern NN applications through composing
        an approximation of the observed Fisher matrix in two low-cost submatrices. But Shampoo isn't 
        only well grounded theoretically in methods for incorporating second-order infromation 
        into our model update rule; there's evidence which shows that Shampoo outperforms both 
        SGD and Adam on Deep CNN and Transformer models.</p>

        <p>Shampoo's computational overhead only narrowly falls above that of SGD and Adam, 
        meaning that's it manages to in large part sidestep the traditional issue with 
        Second-Order optimizers of causing prohibitive runtime / computational costs 
        while still preserving a second-order convergence rate.</p>

        <p>But, that's decidedly not where the story of Shampoo ends as the version of
        Shampoo turning heads like mine nowadays reaches beyond this basic update rule into 
        the complicated world of distributed computing to bring down runtime even further.</p>

        <p>The distributed algorithm for Shampoo utilizes multiple
        worker GPUs, each assigned a subset of the search directions with respect to 
        each parameter, collects up all those gradients and split up again amongst multiple
        worker GPUs for updating the individual parameters according to that pooled gradient.
        Together this implementation reduces the runtime of Shampoo down to 
        only about 10% more than implementations of first-order optimizers, breaking the 
        barrier of second-order approximators significant runtime bottleneck
        <a href="https://arxiv.org/pdf/2309.06497">(Shi et al 2023)</a>.</p>

        <p>That near-equivalent runtime per iteration, combined with increased convergence 
        rate, translates into the distributed implementation of Shampoo having been 
        measured to yield a \(\times 1.35 \) improvement in wall-clock time to achieve 
        validation accuracy over SGD and Adam type alternatives. A different experiment 
        looking at machine translation found that distributed Shampoo reached the particular 
        log-perplexity of that dataset in \(40\% \) less wall-clock time than Adam and AdaGrad
        <a href="https://arxiv.org/pdf/2002.09018">(Anil et al. 2021)</a>,
        largely on the back of the minimally higher iteration runtime and \(\times 1.95 \)
        faster convergence in stepcount to reach that ideal perplexity. </p>

        <div class="centered-item-holder">
            <img
            src="../pics/Screenshot 2024-08-19 at 6.13.04â€¯PM.png"
            alt=""
            class="responsive-image-med"
            />
            <p class="small-text responsive-text-med">
            Accuracy of Shampoo vs. Adam vs. AdaGrad on 93.3M parameter Transformer 
            (6 encoder & decoder layers, 512 model dimension, 2048 hidden dimension, 
            8 attention heads)
            <a
                href=https://arxiv.org/pdf/2002.09018"
                >(Source)</a
            >
            </p>
        </div>

        <p>It's pretty blazingly fast.</p>

        <p>Whether Shampoo will continue to rise to go-to choice for industrial purposes
        and benchmark summitting, looking into how it works and why can lead to some real 
        interesting insights into the math behind NNs and the nature of what model training
        is at a mechanistic level. I hope you've learned something as I know I have. </p>

        <p>Until next time.</p>
    </main>

    <footer>
        <p>All opinions are my own and do not represent those of a employer.</p>
    </footer>

  </body>
</html>
